{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers around the world compete with each other to build the most accurate and capable image recognition systems. So instead of them bending their own neural network designs from scratch, it often makes sense to reuse an existing neural network design as a starting point for your own projects. But even better, researchers also trained these neural network designs on large data sets and share the trained versions of the neural networks. So we can take those pre trained neural networks and either reuse them directly, or use them as a starting point for our own training. \n",
    "\n",
    "Keras the library includes copies of many popular pre trained neural networks that are ready to use. The image recognition models included with Keras are all trained to recognize images from the ImageNet data set. The ImageNet data set is a collection of millions of pictures of objects that have been labeled so that you can use them to train computers to recognize those objects. The date set includes over 1200 pictures of just this specific kind of apple. Let's talk about the neural network designs included with Kares that we can reuse. \n",
    "\n",
    "First is **VGG**, VGG is a deep neural network with either 16 or 19 layers. It was the state of the art in 2014. It's a very standard convolutional neural network design. It's still used widely today as a basis for other models because it's easy to work with and easy to understand. But newer designs tend to be more efficient. **ResNet-50** is a state of the art from 2015, it's a 50 layer neural network that manages to be more accurate, but use less memory than the VGG design. ResNet uses a more complicated design, where higher layers in the neural network are connected not just the layer directly below them, but they also have multiple connections to deeper layers. **Inception v3** is another design from 2015 that also performs very well. It has an even more complex design based around layers that branch out into multiple separate paths before rejoining. \n",
    "\n",
    "These networks show the research trends in 2014 and 2015 to make neural networks bigger and more complex that try to increase their accuracy. More recent neural network designs tend to be more specialized. For example, **Google's MobileNet** created in 2017 is designed specifically to be able to run well on low power devices. The idea was to create a neural network that could run quickly on a cell phone without using too much power while still maintaining a decent level of accuracy. **Google's NASNet** which was created at the end of 2017, explores the idea of having algorithms design neural networks. \n",
    "\n",
    "Having access to these pre trained models is useful for two reasons. First you can reuse any of these models directly in your own programs to recognize objects and images. If you need the ability to recognize any of the 1,000 types of object they're already trained on, you're problems already solved. Second if you want to recognize a different type of object that's not in the 1,000 object training set, it's much faster to start with a pre trained neural network and adapt it to your needs, instead of training your own model from scratch. \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Import libraries\n",
    "All the pretrained models included with Keras are under the applications package. Import the model by saying from Keras applications import vgg16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Image recognition pretrained (VGG16)\n",
    "The first time we run this code, Keras will connect to the internet and download the latest version of the vgg16 model. This means that you'll need internet access to run it, and around 100 megabytes of data will be downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nitsuga/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Top predictions for this image:\n",
      "Prediction: seashore - 0.395212\n",
      "Prediction: promontory - 0.326130\n",
      "Prediction: lakeside - 0.119613\n",
      "Prediction: breakwater - 0.062801\n",
      "Prediction: sandbar - 0.045267\n",
      "Prediction: cliff - 0.011845\n",
      "Prediction: dock - 0.009196\n",
      "Prediction: boathouse - 0.003278\n",
      "Prediction: valley - 0.003194\n"
     ]
    }
   ],
   "source": [
    "# Load Keras' VGG16 model that was pre-trained against the ImageNet database\n",
    "model = vgg16.VGG16()\n",
    "\n",
    "# Load the image file, resizing it to 224x224 pixels (required by VGG model)\n",
    "img = image.load_img(\"bay.jpg\", target_size=(224, 224))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Add a fourth dimension (since Keras expects a list of images)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Normalize the input image's pixel values to the range used when training the neural network\n",
    "x = vgg16.preprocess_input(x)\n",
    "\n",
    "# Run the image through the deep neural network to make a prediction\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Look up the names of the predicted classes. Index zero is the results for the first image.\n",
    "predicted_classes = vgg16.decode_predictions(predictions, top=9)\n",
    "\n",
    "print(\"Top predictions for this image:\")\n",
    "\n",
    "for imagenet_id, name, likelihood in predicted_classes[0]:\n",
    "    print(\"Prediction: {} - {:2f}\".format(name, likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Transfer learning\n",
    "We can use transfer learning to reuse an existing neural network and adapt it to a new problem. Transfer learning is where you take a model trained on one set of data and then use the knowledge it learned to give it a headstart when solving a new problem. \n",
    "\n",
    "The first step is to build a feature extractor that can extract training features from our images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['y_train.dat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to folders with training data\n",
    "dog_path = Path(\"training_data\") / \"dogs\"\n",
    "not_dog_path = Path(\"training_data\") / \"not_dogs\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load all the not-dog images\n",
    "for img in not_dog_path.glob(\"*.png\"):\n",
    "    # Load the image from disk\n",
    "    img = image.load_img(img)\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    image_array = image.img_to_array(img)\n",
    "\n",
    "    # Add the image to the list of images\n",
    "    images.append(image_array)\n",
    "\n",
    "    # For each 'not dog' image, the expected value should be 0\n",
    "    labels.append(0)\n",
    "\n",
    "# Load all the dog images\n",
    "for img in dog_path.glob(\"*.png\"):\n",
    "    # Load the image from disk\n",
    "    img = image.load_img(img)\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    image_array = image.img_to_array(img)\n",
    "\n",
    "    # Add the image to the list of images\n",
    "    images.append(image_array)\n",
    "\n",
    "    # For each 'dog' image, the expected value should be 1\n",
    "    labels.append(1)\n",
    "\n",
    "# Create a single numpy array with all the images we loaded\n",
    "x_train = np.array(images)\n",
    "\n",
    "# Also convert the labels to a numpy array\n",
    "y_train = np.array(labels)\n",
    "\n",
    "# Normalize image data to 0-to-1 range\n",
    "x_train = vgg16.preprocess_input(x_train)\n",
    "\n",
    "# Load a pre-trained neural network to use as a feature extractor\n",
    "pretrained_nn = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Extract features for each image (all in one pass)\n",
    "features_x = pretrained_nn.predict(x_train)\n",
    "\n",
    "# Save the array of extracted features to a file\n",
    "joblib.dump(features_x, \"x_train.dat\")\n",
    "\n",
    "# Save the matching array of expected values to a file\n",
    "joblib.dump(y_train, \"y_train.dat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used the pre trained neural network to extract features from our training images. Now we're ready to train a new neural network that uses those extracted features. Instead of loading raw images to train with, we're gonna load the features that we extracted with the pre trained VGG 16 neural network. Since we use VGG 16 to extract features from our image, this neural network has no convolutional layers. Instead it only has the final dense layers of the neural network. These are the only layers that we'll be retraining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nitsuga/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/nitsuga/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 55ms/step - loss: 4.5036 - acc: 0.6034\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 0s 902us/step - loss: 1.8010 - acc: 0.8621\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 0s 971us/step - loss: 1.2681 - acc: 0.8966\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.8246 - acc: 0.9483\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.9564 - acc: 0.9310\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.8246 - acc: 0.9483\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.8326 - acc: 0.9483\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.8246 - acc: 0.9483\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 0s 792us/step - loss: 0.8246 - acc: 0.9483\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.8246 - acc: 0.9483\n"
     ]
    }
   ],
   "source": [
    "# Load data set\n",
    "x_train = joblib.load(\"x_train.dat\")\n",
    "y_train = joblib.load(\"y_train.dat\")\n",
    "\n",
    "# Create a model and add layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Save neural network structure\n",
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)\n",
    "\n",
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used transfer learning to create and train a neural network that can recognize pictures of dogs. Let's see how they use that neural network to make predictions. We'll need the pre processor image with the vgg16 feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood that this image contains a dog: 0%\n"
     ]
    }
   ],
   "source": [
    "# Load the json file that contains the model's structure\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()\n",
    "\n",
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)\n",
    "\n",
    "# Re-load the model's trained weights\n",
    "model.load_weights(\"model_weights.h5\")\n",
    "\n",
    "# Load an image file to test, resizing it to 64x64 pixels (as required by this model)\n",
    "img = image.load_img(\"not_dog.png\", target_size=(64, 64))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "image_array = image.img_to_array(img)\n",
    "\n",
    "# Add a forth dimension to the image (since Keras expects a bunch of images, not a single image)\n",
    "images = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Normalize the data\n",
    "images = vgg16.preprocess_input(images)\n",
    "\n",
    "# Use the pre-trained neural network to extract features from our test image (the same way we did to train the model)\n",
    "feature_extraction_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "features = feature_extraction_model.predict(images)\n",
    "\n",
    "# Given the extracted features, make a final prediction using our own model\n",
    "results = model.predict(features)\n",
    "\n",
    "# Since we are only testing one image with possible class, we only need to check the first result's first element\n",
    "single_result = results[0][0]\n",
    "\n",
    "# Print the result\n",
    "print(\"Likelihood that this image contains a dog: {}%\".format(int(single_result * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood that this image contains a dog: 100%\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img(\"dog.png\", target_size=(64, 64))\n",
    "image_array = image.img_to_array(img)\n",
    "images = np.expand_dims(image_array, axis=0)\n",
    "images = vgg16.preprocess_input(images)\n",
    "feature_extraction_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "features = feature_extraction_model.predict(images)\n",
    "results = model.predict(features)\n",
    "single_result = results[0][0]\n",
    "print(\"Likelihood that this image contains a dog: {}%\".format(int(single_result * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood that this image contains a dog: 0%\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img(\"bay.jpg\", target_size=(64, 64))\n",
    "image_array = image.img_to_array(img)\n",
    "images = np.expand_dims(image_array, axis=0)\n",
    "images = vgg16.preprocess_input(images)\n",
    "feature_extraction_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "features = feature_extraction_model.predict(images)\n",
    "results = model.predict(features)\n",
    "single_result = results[0][0]\n",
    "print(\"Likelihood that this image contains a dog: {}%\".format(int(single_result * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
